{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Stack RAG Lifecycle\n",
    "\n",
    "In this notebook, we will walk through the lifecycle of building and evaluating a RAG pipeline using Llama Stack. \n",
    "\n",
    "**Example: Torchtune Knowledge Agent** \n",
    "\n",
    "Throughout this notebook, we will build a knowledge agent that can answer questions about the Torchtune project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-stack-client==0.2.1 --quiet\n",
    "!pip install llama-toolchain --quiet\n",
    "!pip install llama-stack --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient, Agent\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
    "from rich.pretty import pprint\n",
    "import json\n",
    "import uuid\n",
    "from pydantic import BaseModel\n",
    "import rich\n",
    "import os\n",
    "# try:\n",
    "#     from google.colab import userdata\n",
    "#     os.environ['FIREWORKS_API_KEY'] = userdata.get('FIREWORKS_API_KEY')\n",
    "# except ImportError:\n",
    "#     print(\"Not in Google Colab environment\")\n",
    "\n",
    "# client = LlamaStackAsLibraryClient(\"fireworks\", provider_data = {\"fireworks_api_key\": os.environ['FIREWORKS_API_KEY']})\n",
    "# _ = client.initialize()\n",
    "\n",
    "# Uncomment to run on a hosted Llama Stack server\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "# MODEL_ID = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# MODEL_ID = \"mistral\"\n",
    "MODEL_ID=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Vanilla Agent\n",
    "\n",
    "First, we will build a simple vanilla agent without any access to external knowledge base or tools, and check how it performs on a couple of questions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's come up with a couple of examples to test the agent\n",
    "examples = [\n",
    "    {\n",
    "        \"input_query\": \"What precision formats does torchtune support?\",\n",
    "        \"expected_answer\": \"Torchtune supports two data types for precision: fp32 (full-precision) which uses 4 bytes per model and optimizer parameter, and bfloat16 (half-precision) which uses 2 bytes per model and optimizer parameter.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What does DoRA stand for in torchtune?\",\n",
    "        \"expected_answer\": \"Weight-Decomposed Low-Rank Adaptation\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"How does the CPUOffloadOptimizer reduce GPU memory usage?\",\n",
    "        \"expected_answer\": \"The CPUOffloadOptimizer reduces GPU memory usage by keeping optimizer states on CPU and performing optimizer steps on CPU. It can also optionally offload gradients to CPU by using offload_gradients=True\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"How do I ensure only LoRA parameters are trainable when fine-tuning?\",\n",
    "        \"expected_answer\": \"You can set only LoRA parameters to trainable using torchtune's utility functions: first fetch all LoRA parameters with lora_params = get_adapter_params(lora_model), then set them as trainable with set_trainable_params(lora_model, lora_params). The LoRA recipe handles this automatically.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Question:</span> What precision formats does torchtune support?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mQuestion:\u001b[0m What precision formats does torchtune support?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Agent Answer:</span> Okay, so I'm trying to figure out what precision formats Torchtune supports. I remember that \n",
       "precision in machine learning usually refers to the data type used to store numbers, which affects both memory \n",
       "usage and computation speed. Common formats I've heard about are float32, float16, and maybe even some others like \n",
       "bfloat16 or int8.\n",
       "\n",
       "I think Torchtune is a library related to PyTorch, probably for model tuning or optimization. Since it's built on \n",
       "PyTorch, it might leverage PyTorch's native data types. I know PyTorch supports float32, float16, and bfloat16, \n",
       "especially on GPUs. Float16 uses less memory, which is good for large models, but it has less precision. Bfloat16 \n",
       "is another <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>-bit format that's used in some hardware, like Google's TPU.\n",
       "\n",
       "Wait, does Torchtune support all these? Or maybe it has its own way of handling precision? I should check the \n",
       "documentation or the project's official sources. But since I don't have access to that right now, I'll have to rely\n",
       "on what I know about PyTorch and similar libraries.\n",
       "\n",
       "I also remember that some libraries support mixed precision training, where parts of the model use different \n",
       "precisions. For example, using float16 for computation and float32 for certain layers to maintain stability. \n",
       "Torchtune might support this kind of setup.\n",
       "\n",
       "Another thought: sometimes libraries support quantization, which is a form of lower precision. Quantization can be \n",
       "done with <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>-bit integers <span style=\"font-weight: bold\">(</span>int8<span style=\"font-weight: bold\">)</span> or other lower bit-width types. If Torchtune supports quantization, that would add \n",
       "more precision formats to its support list.\n",
       "\n",
       "But I'm not sure if Torchtune specifically supports int8 or other quantized types. It might depend on the version \n",
       "or the specific features implemented. I should consider that Torchtune might rely on PyTorch's native types, so if \n",
       "PyTorch supports a certain precision, Torchtune likely does too.\n",
       "\n",
       "In summary, I think Torchtune supports float32, float16, and bfloat16, possibly with mixed precision training. It \n",
       "might also support quantization types like int8 if that's part of PyTorch's capabilities. I should probably mention\n",
       "that checking the official documentation would give the most accurate and up-to-date information.\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">think</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "The Torchtune project supports several precision formats, leveraging the capabilities of PyTorch. Here's a summary \n",
       "of the supported precision formats:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Float32**: This is the standard <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>-bit floating-point precision, widely used for its balance between precision\n",
       "and memory usage.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Float16**: A <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>-bit floating-point format that uses half the memory of float32, making it suitable for large \n",
       "models. It offers lower precision but can be beneficial for training and inference speed.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **BFloat16**: Another <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>-bit format that is supported by certain hardware, such as Google's TPU. It is useful \n",
       "for maintaining numerical stability in lower precision computations.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Mixed Precision Training**: Torchtune likely supports configurations where different parts of the model use \n",
       "varying precisions, such as using float16 for computation and float32 for specific layers to maintain stability.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Quantization**: Depending on the version and implementation, Torchtune might support quantization techniques, \n",
       "which use lower bit-width types like int8 to reduce memory usage and improve performance.\n",
       "\n",
       "For the most accurate and up-to-date information, consulting the official Torchtune documentation is recommended.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mAgent Answer:\u001b[0m Okay, so I'm trying to figure out what precision formats Torchtune supports. I remember that \n",
       "precision in machine learning usually refers to the data type used to store numbers, which affects both memory \n",
       "usage and computation speed. Common formats I've heard about are float32, float16, and maybe even some others like \n",
       "bfloat16 or int8.\n",
       "\n",
       "I think Torchtune is a library related to PyTorch, probably for model tuning or optimization. Since it's built on \n",
       "PyTorch, it might leverage PyTorch's native data types. I know PyTorch supports float32, float16, and bfloat16, \n",
       "especially on GPUs. Float16 uses less memory, which is good for large models, but it has less precision. Bfloat16 \n",
       "is another \u001b[1;36m16\u001b[0m-bit format that's used in some hardware, like Google's TPU.\n",
       "\n",
       "Wait, does Torchtune support all these? Or maybe it has its own way of handling precision? I should check the \n",
       "documentation or the project's official sources. But since I don't have access to that right now, I'll have to rely\n",
       "on what I know about PyTorch and similar libraries.\n",
       "\n",
       "I also remember that some libraries support mixed precision training, where parts of the model use different \n",
       "precisions. For example, using float16 for computation and float32 for certain layers to maintain stability. \n",
       "Torchtune might support this kind of setup.\n",
       "\n",
       "Another thought: sometimes libraries support quantization, which is a form of lower precision. Quantization can be \n",
       "done with \u001b[1;36m8\u001b[0m-bit integers \u001b[1m(\u001b[0mint8\u001b[1m)\u001b[0m or other lower bit-width types. If Torchtune supports quantization, that would add \n",
       "more precision formats to its support list.\n",
       "\n",
       "But I'm not sure if Torchtune specifically supports int8 or other quantized types. It might depend on the version \n",
       "or the specific features implemented. I should consider that Torchtune might rely on PyTorch's native types, so if \n",
       "PyTorch supports a certain precision, Torchtune likely does too.\n",
       "\n",
       "In summary, I think Torchtune supports float32, float16, and bfloat16, possibly with mixed precision training. It \n",
       "might also support quantization types like int8 if that's part of PyTorch's capabilities. I should probably mention\n",
       "that checking the official documentation would give the most accurate and up-to-date information.\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mthink\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "The Torchtune project supports several precision formats, leveraging the capabilities of PyTorch. Here's a summary \n",
       "of the supported precision formats:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Float32**: This is the standard \u001b[1;36m32\u001b[0m-bit floating-point precision, widely used for its balance between precision\n",
       "and memory usage.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Float16**: A \u001b[1;36m16\u001b[0m-bit floating-point format that uses half the memory of float32, making it suitable for large \n",
       "models. It offers lower precision but can be beneficial for training and inference speed.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **BFloat16**: Another \u001b[1;36m16\u001b[0m-bit format that is supported by certain hardware, such as Google's TPU. It is useful \n",
       "for maintaining numerical stability in lower precision computations.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Mixed Precision Training**: Torchtune likely supports configurations where different parts of the model use \n",
       "varying precisions, such as using float16 for computation and float32 for specific layers to maintain stability.\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. **Quantization**: Depending on the version and implementation, Torchtune might support quantization techniques, \n",
       "which use lower bit-width types like int8 to reduce memory usage and improve performance.\n",
       "\n",
       "For the most accurate and up-to-date information, consulting the official Torchtune documentation is recommended.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Question:</span> What does DoRA stand for in torchtune?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mQuestion:\u001b[0m What does DoRA stand for in torchtune?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Agent Answer:</span> Okay, so I'm trying to figure out what DoRA stands for in the context of the Torchtune project. I \n",
       "know that Torchtune is an open-source library for automated machine learning <span style=\"font-weight: bold\">(</span>AutoML<span style=\"font-weight: bold\">)</span> with a focus on deep \n",
       "learning, built on top of PyTorch. But I'm not exactly sure what DoRA refers to here.\n",
       "\n",
       "First, I'll break down the term <span style=\"color: #008000; text-decoration-color: #008000\">\"DoRA.\"</span> It could be an acronym, so I'll consider each letter: D, O, R, A. Maybe \n",
       "it's something like <span style=\"color: #008000; text-decoration-color: #008000\">\"Dynamic Resource Allocation\"</span> or <span style=\"color: #008000; text-decoration-color: #008000\">\"Distributed Optimization and Resource Allocation.\"</span> But I'm \n",
       "not sure if that's accurate.\n",
       "\n",
       "I remember that in AutoML, resource allocation is a key aspect, especially when dealing with computational \n",
       "resources like CPU and GPU. So DoRA might relate to how resources are managed during the training process. Maybe \n",
       "it's a feature in Torchtune that dynamically adjusts resources based on the model's needs.\n",
       "\n",
       "I should check if there's any documentation or official sources from the Torchtune project that mention DoRA. If I \n",
       "look up Torchtune's GitHub repository or their documentation, I might find more information. Alternatively, \n",
       "searching for <span style=\"color: #008000; text-decoration-color: #008000\">\"DoRA Torchtune\"</span> could give me some clues.\n",
       "\n",
       "Wait, I think I've heard the term DoRA in the context of distributed training and resource management. Maybe it's \n",
       "about dynamically allocating resources to different parts of the model or across different workers in a distributed\n",
       "environment. That would make sense because efficient resource allocation can improve training efficiency and \n",
       "scalability.\n",
       "\n",
       "Another angle: sometimes in AutoML, especially with hyperparameter tuning, the system needs to allocate resources \n",
       "<span style=\"font-weight: bold\">(</span>like compute time<span style=\"font-weight: bold\">)</span> to different trials. DoRA could be the mechanism that decides how much resources each trial \n",
       "gets based on their progress or potential.\n",
       "\n",
       "I should also consider that DoRA might be a specific module or component within Torchtune. Perhaps it's part of the\n",
       "tuning process where resources are dynamically adjusted to optimize the training process. For example, if a model \n",
       "isn't using all available GPUs efficiently, DoRA might redistribute resources to balance the load.\n",
       "\n",
       "I'm trying to recall if I've seen any examples or tutorials where DoRA is mentioned. Maybe in a blog post or a \n",
       "paper about Torchtune. If I can't find exact information, I might have to make an educated guess based on common \n",
       "AutoML concepts.\n",
       "\n",
       "Putting it all together, DoRA in Torchtune likely stands for Dynamic Resource Allocation. It probably refers to the\n",
       "system's ability to dynamically adjust computational resources during the AutoML process to optimize training \n",
       "efficiency and resource utilization. This would help in scaling the AutoML process and making it more adaptable to \n",
       "different workloads and environments.\n",
       "\n",
       "I think I'm on the right track, but I'm not <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>% certain. To confirm, I should look for any official Torchtune \n",
       "documentation or releases that mention DoRA. If I can't find it, I might have to rely on the context and common \n",
       "AutoML practices to define it accurately.\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">think</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "DoRA in the context of the Torchtune project stands for Dynamic Resource Allocation. It refers to the system's \n",
       "capability to dynamically adjust computational resources during the automated machine learning <span style=\"font-weight: bold\">(</span>AutoML<span style=\"font-weight: bold\">)</span> process. \n",
       "This feature optimizes training efficiency and resource utilization, enhancing scalability and adaptability in \n",
       "different workloads and environments.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mAgent Answer:\u001b[0m Okay, so I'm trying to figure out what DoRA stands for in the context of the Torchtune project. I \n",
       "know that Torchtune is an open-source library for automated machine learning \u001b[1m(\u001b[0mAutoML\u001b[1m)\u001b[0m with a focus on deep \n",
       "learning, built on top of PyTorch. But I'm not exactly sure what DoRA refers to here.\n",
       "\n",
       "First, I'll break down the term \u001b[32m\"DoRA.\"\u001b[0m It could be an acronym, so I'll consider each letter: D, O, R, A. Maybe \n",
       "it's something like \u001b[32m\"Dynamic Resource Allocation\"\u001b[0m or \u001b[32m\"Distributed Optimization and Resource Allocation.\"\u001b[0m But I'm \n",
       "not sure if that's accurate.\n",
       "\n",
       "I remember that in AutoML, resource allocation is a key aspect, especially when dealing with computational \n",
       "resources like CPU and GPU. So DoRA might relate to how resources are managed during the training process. Maybe \n",
       "it's a feature in Torchtune that dynamically adjusts resources based on the model's needs.\n",
       "\n",
       "I should check if there's any documentation or official sources from the Torchtune project that mention DoRA. If I \n",
       "look up Torchtune's GitHub repository or their documentation, I might find more information. Alternatively, \n",
       "searching for \u001b[32m\"DoRA Torchtune\"\u001b[0m could give me some clues.\n",
       "\n",
       "Wait, I think I've heard the term DoRA in the context of distributed training and resource management. Maybe it's \n",
       "about dynamically allocating resources to different parts of the model or across different workers in a distributed\n",
       "environment. That would make sense because efficient resource allocation can improve training efficiency and \n",
       "scalability.\n",
       "\n",
       "Another angle: sometimes in AutoML, especially with hyperparameter tuning, the system needs to allocate resources \n",
       "\u001b[1m(\u001b[0mlike compute time\u001b[1m)\u001b[0m to different trials. DoRA could be the mechanism that decides how much resources each trial \n",
       "gets based on their progress or potential.\n",
       "\n",
       "I should also consider that DoRA might be a specific module or component within Torchtune. Perhaps it's part of the\n",
       "tuning process where resources are dynamically adjusted to optimize the training process. For example, if a model \n",
       "isn't using all available GPUs efficiently, DoRA might redistribute resources to balance the load.\n",
       "\n",
       "I'm trying to recall if I've seen any examples or tutorials where DoRA is mentioned. Maybe in a blog post or a \n",
       "paper about Torchtune. If I can't find exact information, I might have to make an educated guess based on common \n",
       "AutoML concepts.\n",
       "\n",
       "Putting it all together, DoRA in Torchtune likely stands for Dynamic Resource Allocation. It probably refers to the\n",
       "system's ability to dynamically adjust computational resources during the AutoML process to optimize training \n",
       "efficiency and resource utilization. This would help in scaling the AutoML process and making it more adaptable to \n",
       "different workloads and environments.\n",
       "\n",
       "I think I'm on the right track, but I'm not \u001b[1;36m100\u001b[0m% certain. To confirm, I should look for any official Torchtune \n",
       "documentation or releases that mention DoRA. If I can't find it, I might have to rely on the context and common \n",
       "AutoML practices to define it accurately.\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mthink\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "DoRA in the context of the Torchtune project stands for Dynamic Resource Allocation. It refers to the system's \n",
       "capability to dynamically adjust computational resources during the automated machine learning \u001b[1m(\u001b[0mAutoML\u001b[1m)\u001b[0m process. \n",
       "This feature optimizes training efficiency and resource utilization, enhancing scalability and adaptability in \n",
       "different workloads and environments.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Question:</span> How does the CPUOffloadOptimizer reduce GPU memory usage?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mQuestion:\u001b[0m How does the CPUOffloadOptimizer reduce GPU memory usage?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Agent Answer:</span> Okay, so I'm trying to understand how the CPUOffloadOptimizer reduces GPU memory usage in the \n",
       "Torchtune project. I remember that in machine learning, especially with deep learning models, GPU memory is a \n",
       "critical resource because models can be really large. If the GPU runs out of memory, training can't proceed, which \n",
       "is a problem.\n",
       "\n",
       "I think the CPUOffloadOptimizer must have something to do with moving some parts of the computation from the GPU to\n",
       "the CPU. But how exactly does that help with memory? Well, if certain parts of the model or data are moved to the \n",
       "CPU, that should free up some GPU memory. But I'm not entirely sure about the specifics.\n",
       "\n",
       "Let me break it down. When training a model, all the model parameters and the data batches are usually kept on the \n",
       "GPU because GPUs are faster for these operations. But if the model is too big, even keeping all the parameters on \n",
       "the GPU can cause it to run out of memory. So, maybe the CPUOffloadOptimizer moves some of these parameters to the \n",
       "CPU when needed.\n",
       "\n",
       "I've heard of gradient checkpointing before. I think that's a technique where instead of keeping all the \n",
       "intermediate activations during the forward pass, you compute them on the fly and store just enough to compute the \n",
       "gradients. This reduces the memory needed during backpropagation. So, maybe the CPUOffloadOptimizer uses gradient \n",
       "checkpointing to save memory.\n",
       "\n",
       "Another thing I remember is that some layers can be offloaded. If certain layers are moved to the CPU, their \n",
       "computations would happen on the CPU, which is slower but frees up GPU memory. So, the optimizer might identify \n",
       "which layers can be moved without causing too much slowdown and offload them.\n",
       "\n",
       "I also think about mixed precision training. This uses lower-precision <span style=\"font-weight: bold\">(</span>like <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>-bit<span style=\"font-weight: bold\">)</span> computations to save memory, \n",
       "but I'm not sure if that's part of the CPUOffloadOptimizer or a separate technique. Maybe the optimizer doesn't \n",
       "handle that directly but focuses more on moving data and computations.\n",
       "\n",
       "Putting it all together, the CPUOffloadOptimizer probably works by moving parts of the model or computations to the\n",
       "CPU when the GPU is running out of memory. It might use techniques like gradient checkpointing and layer offloading\n",
       "to free up space. It's a strategy to balance computation speed and memory usage, ensuring that training can \n",
       "continue without running out of GPU memory.\n",
       "\n",
       "I should also consider how this affects training time. Moving computations to the CPU is slower, so there's a \n",
       "trade-off between memory usage and speed. The optimizer likely tries to find the right balance to keep training \n",
       "efficient while preventing memory issues.\n",
       "\n",
       "In summary, the CPUOffloadOptimizer reduces GPU memory by offloading some computations and data to the CPU, \n",
       "possibly using techniques like gradient checkpointing and layer offloading, thus allowing training to proceed \n",
       "without exceeding GPU memory limits.\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">think</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "The CPUOffloadOptimizer in the Torchtune project reduces GPU memory usage by strategically offloading certain \n",
       "computations and data to the CPU, thereby optimizing memory usage without significantly compromising training \n",
       "efficiency. Here's a detailed explanation of how it works:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Computation Offloading**: The optimizer identifies parts of the model or computations that can be moved from \n",
       "the GPU to the CPU. This offloading frees up GPU memory, allowing training to proceed even with large models.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Gradient Checkpointing**: This technique reduces memory usage during backpropagation by recomputing \n",
       "intermediate activations instead of storing them. This minimizes the memory footprint during the backward pass.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Layer Offloading**: The optimizer may move specific layers to the CPU. While this slows down those \n",
       "computations, it effectively frees up GPU memory, enabling training to continue without memory issues.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Balancing Efficiency and Memory**: The CPUOffloadOptimizer seeks a balance between computation speed and \n",
       "memory usage. It aims to keep training efficient while preventing GPU memory overload, ensuring smooth model \n",
       "training.\n",
       "\n",
       "In summary, the CPUOffloadOptimizer optimizes GPU memory by offloading computations and data to the CPU, using \n",
       "techniques like gradient checkpointing and layer offloading, thus maintaining training efficiency while avoiding \n",
       "memory constraints.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mAgent Answer:\u001b[0m Okay, so I'm trying to understand how the CPUOffloadOptimizer reduces GPU memory usage in the \n",
       "Torchtune project. I remember that in machine learning, especially with deep learning models, GPU memory is a \n",
       "critical resource because models can be really large. If the GPU runs out of memory, training can't proceed, which \n",
       "is a problem.\n",
       "\n",
       "I think the CPUOffloadOptimizer must have something to do with moving some parts of the computation from the GPU to\n",
       "the CPU. But how exactly does that help with memory? Well, if certain parts of the model or data are moved to the \n",
       "CPU, that should free up some GPU memory. But I'm not entirely sure about the specifics.\n",
       "\n",
       "Let me break it down. When training a model, all the model parameters and the data batches are usually kept on the \n",
       "GPU because GPUs are faster for these operations. But if the model is too big, even keeping all the parameters on \n",
       "the GPU can cause it to run out of memory. So, maybe the CPUOffloadOptimizer moves some of these parameters to the \n",
       "CPU when needed.\n",
       "\n",
       "I've heard of gradient checkpointing before. I think that's a technique where instead of keeping all the \n",
       "intermediate activations during the forward pass, you compute them on the fly and store just enough to compute the \n",
       "gradients. This reduces the memory needed during backpropagation. So, maybe the CPUOffloadOptimizer uses gradient \n",
       "checkpointing to save memory.\n",
       "\n",
       "Another thing I remember is that some layers can be offloaded. If certain layers are moved to the CPU, their \n",
       "computations would happen on the CPU, which is slower but frees up GPU memory. So, the optimizer might identify \n",
       "which layers can be moved without causing too much slowdown and offload them.\n",
       "\n",
       "I also think about mixed precision training. This uses lower-precision \u001b[1m(\u001b[0mlike \u001b[1;36m16\u001b[0m-bit\u001b[1m)\u001b[0m computations to save memory, \n",
       "but I'm not sure if that's part of the CPUOffloadOptimizer or a separate technique. Maybe the optimizer doesn't \n",
       "handle that directly but focuses more on moving data and computations.\n",
       "\n",
       "Putting it all together, the CPUOffloadOptimizer probably works by moving parts of the model or computations to the\n",
       "CPU when the GPU is running out of memory. It might use techniques like gradient checkpointing and layer offloading\n",
       "to free up space. It's a strategy to balance computation speed and memory usage, ensuring that training can \n",
       "continue without running out of GPU memory.\n",
       "\n",
       "I should also consider how this affects training time. Moving computations to the CPU is slower, so there's a \n",
       "trade-off between memory usage and speed. The optimizer likely tries to find the right balance to keep training \n",
       "efficient while preventing memory issues.\n",
       "\n",
       "In summary, the CPUOffloadOptimizer reduces GPU memory by offloading some computations and data to the CPU, \n",
       "possibly using techniques like gradient checkpointing and layer offloading, thus allowing training to proceed \n",
       "without exceeding GPU memory limits.\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mthink\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "The CPUOffloadOptimizer in the Torchtune project reduces GPU memory usage by strategically offloading certain \n",
       "computations and data to the CPU, thereby optimizing memory usage without significantly compromising training \n",
       "efficiency. Here's a detailed explanation of how it works:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Computation Offloading**: The optimizer identifies parts of the model or computations that can be moved from \n",
       "the GPU to the CPU. This offloading frees up GPU memory, allowing training to proceed even with large models.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Gradient Checkpointing**: This technique reduces memory usage during backpropagation by recomputing \n",
       "intermediate activations instead of storing them. This minimizes the memory footprint during the backward pass.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Layer Offloading**: The optimizer may move specific layers to the CPU. While this slows down those \n",
       "computations, it effectively frees up GPU memory, enabling training to continue without memory issues.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Balancing Efficiency and Memory**: The CPUOffloadOptimizer seeks a balance between computation speed and \n",
       "memory usage. It aims to keep training efficient while preventing GPU memory overload, ensuring smooth model \n",
       "training.\n",
       "\n",
       "In summary, the CPUOffloadOptimizer optimizes GPU memory by offloading computations and data to the CPU, using \n",
       "techniques like gradient checkpointing and layer offloading, thus maintaining training efficiency while avoiding \n",
       "memory constraints.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Question:</span> How do I ensure only LoRA parameters are trainable when fine-tuning?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mQuestion:\u001b[0m How do I ensure only LoRA parameters are trainable when fine-tuning?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Agent Answer:</span> Okay, so I'm trying to figure out how to ensure that only the LoRA parameters are trainable when I'm \n",
       "fine-tuning a model using the Torchtune project. I remember that LoRA stands for Low-Rank Adaptation, and it's a \n",
       "method used to efficiently fine-tune large language models by modifying only a small number of parameters. This is \n",
       "supposed to make the fine-tuning process faster and less resource-intensive compared to fine-tuning the entire \n",
       "model.\n",
       "\n",
       "First, I think I need to understand how LoRA works in the context of Torchtune. From what I recall, LoRA involves \n",
       "adding two small matrices to the attention layers of a transformer model. These matrices are typically low-rank, \n",
       "meaning they have a much smaller dimension than the original layers, which reduces the number of parameters that \n",
       "need to be updated during fine-tuning.\n",
       "\n",
       "So, when using Torchtune, I believe there's a specific way to apply LoRA. Maybe it's through a decorator or a \n",
       "configuration setting. I think the Hugging Face library, which Torchtune is built on, has a LoRA approach, and \n",
       "perhaps Torchtune provides a similar or integrated method.\n",
       "\n",
       "I remember seeing something about using the `LoraConfig` class in Hugging Face's `transformers` library. Maybe \n",
       "Torchtune has a similar configuration option. I should check if Torchtune has a `LoraConfig` or something \n",
       "equivalent. If it does, I can set that up to enable LoRA.\n",
       "\n",
       "Once LoRA is enabled, I need to make sure that only the LoRA parameters are trainable. That means the rest of the \n",
       "model's parameters should be frozen or not updated during training. I think in PyTorch, you can set `requires_grad`\n",
       "to <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> for the parameters you don't want to update. But with LoRA, perhaps Torchtune handles this automatically, \n",
       "so I just need to ensure that the LoRA parameters are the only ones with `requires_grad` set to <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>.\n",
       "\n",
       "I also recall that when using LoRA, you typically use an adapter or a similar mechanism to apply the low-rank \n",
       "transformations. In Torchtune, maybe this is done through a specific trainer or a callback. I should look into the \n",
       "Torchtune documentation to see if there's a recommended way to apply LoRA and control which parameters are \n",
       "trainable.\n",
       "\n",
       "Another thought: when initializing the model with LoRA, the base model's parameters are usually frozen. So, during \n",
       "the fine-tuning process, only the LoRA parameters are updated. I think this is done by setting the base model to \n",
       "evaluation mode or freezing its parameters. Maybe in Torchtune, this is handled by the `pl.Trainer` configuration \n",
       "or through some specific flags.\n",
       "\n",
       "I should also consider the training loop. In PyTorch Lightning, which Torchtune uses, the training loop \n",
       "automatically updates the parameters based on the optimizer. So, if I set up the model correctly with only LoRA \n",
       "parameters trainable, the optimizer should only update those.\n",
       "\n",
       "Wait, maybe I can use the `freeze` method provided by Torchtune or Hugging Face. For example, after applying LoRA, \n",
       "I can freeze the base model and unfreeze the LoRA layers. That way, during training, only the LoRA parameters are \n",
       "updated.\n",
       "\n",
       "I'm a bit confused about how exactly to apply LoRA in Torchtune. Let me think step by step. First, I load the base \n",
       "model, then apply the LoRA adapter to it. Then, I need to make sure that the base model's parameters are not \n",
       "trainable. So, I can loop through the model's named parameters and set `requires_grad` to <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> for all except the \n",
       "LoRA ones.\n",
       "\n",
       "Alternatively, maybe Torchtune provides a utility function to apply LoRA and freeze the base model automatically. I\n",
       "should check the Torchtune documentation or examples to see if there's a standard way to do this.\n",
       "\n",
       "Another point: when using LoRA, the model's forward pass is modified to include the LoRA transformations. So, I \n",
       "need to make sure that during the forward pass, the LoRA layers are active and their parameters are being used. I \n",
       "think this is handled by the LoRA implementation, but I should confirm.\n",
       "\n",
       "I also wonder about the optimizer and learning rate. Since only a small portion of parameters are being updated, \n",
       "maybe I need to adjust the learning rate for those parameters differently. But I think the optimizer in PyTorch \n",
       "Lightning handles this as long as the parameters are marked as trainable.\n",
       "\n",
       "In summary, the steps I think I need to take are:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Load the base model.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Apply LoRA to the model, which adds the LoRA parameters.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Freeze the base model's parameters so they aren't updated during training.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Ensure that only the LoRA parameters are marked as trainable.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Use the appropriate trainer configuration in Torchtune to handle the training loop.\n",
       "\n",
       "I should look up specific code examples or documentation from Torchtune that demonstrates applying LoRA and \n",
       "fine-tuning only those parameters. Maybe there's a specific decorator or function call that handles this setup for \n",
       "me.\n",
       "\n",
       "Wait, I think in Hugging Face's `transformers` library, they have a `set_frozen` method or something similar to \n",
       "freeze parts of the model. Maybe Torchtune has a similar utility. Alternatively, I can manually loop through the \n",
       "parameters and set `requires_grad` as needed.\n",
       "\n",
       "I'm also considering whether using the `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.no_grad</span><span style=\"font-weight: bold\">()</span>` context might help, but I think that's more for inference \n",
       "rather than training. So, probably not the right approach here.\n",
       "\n",
       "Another idea: when using LoRA, the adapter layers are typically added to the attention layers. So, in the model, \n",
       "these adapter layers are the only ones that should be trainable. Therefore, I can iterate over the model's modules \n",
       "and set `requires_grad` to <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> for all modules except the adapter ones.\n",
       "\n",
       "But how do I identify the adapter modules? Maybe they have a specific name or are part of a certain class. I should\n",
       "check the structure of the model after applying LoRA to see how the adapter layers are named or structured.\n",
       "\n",
       "Alternatively, perhaps Torchtune provides a way to automatically handle this, like a flag to enable LoRA and only \n",
       "train those parameters. That would be the most straightforward approach.\n",
       "\n",
       "I think I need to look into the Torchtune documentation or examples to see if there's a built-in method for \n",
       "applying LoRA and controlling trainability. If not, I can fall back to manually setting the parameters' \n",
       "`requires_grad` attribute.\n",
       "\n",
       "In any case, the key points are:\n",
       "\n",
       "- Apply LoRA to the model to add the small, trainable parameters.\n",
       "- Freeze the base model's parameters to prevent them from being updated.\n",
       "- Ensure that only the LoRA parameters are marked as trainable.\n",
       "\n",
       "I should also test this setup by checking the number of trainable parameters before and after training to confirm \n",
       "that only the LoRA parameters are being updated. That way, I can verify that my approach is correct.\n",
       "\n",
       "Another consideration: when using LoRA, sometimes the original attention weights are kept but scaled by the LoRA \n",
       "parameters. So, it's important that the base model's parameters remain unchanged during training, which is why \n",
       "freezing them is necessary.\n",
       "\n",
       "I think I've got a rough idea of the steps. Now, I should try to implement this in code, using Torchtune's APIs, \n",
       "and see if it works as expected. If I run into issues, I can refer back to the documentation or seek help from \n",
       "community resources.\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">think</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "To ensure only LoRA parameters are trainable when fine-tuning using Torchtune, follow these steps:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Load the Base Model**: Start by loading the pre-trained model you wish to fine-tune.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Apply LoRA**: Use Torchtune's or Hugging Face's `LoraConfig` to apply LoRA to the model. This adds LoRA \n",
       "adapter layers to the attention modules.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Freeze the Base Model**: After applying LoRA, freeze the base model's parameters to prevent them from being \n",
       "updated during training. This can be done by setting `requires_grad` to `<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>` for all parameters except the LoRA \n",
       "ones.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Configure the Trainer**: Use Torchtune's trainer with appropriate settings to handle the training loop, \n",
       "ensuring only LoRA parameters are updated.\n",
       "\n",
       "Here's a code example demonstrating these steps:\n",
       "\n",
       "```python\n",
       "from torchtune import Torchtune\n",
       "from transformers import LoraConfig, AutoModelForMaskedLM\n",
       "\n",
       "# Load the base model\n",
       "model = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AutoModelForMaskedLM.from_pretrained</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'bert-base-uncased'</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "# Apply LoRA configuration\n",
       "config = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LoraConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">r</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,  # Rank of the LoRA adaptation\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">lora_attention</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,  # Apply LoRA to attention layers\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">lora_feedforward</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  # Apply LoRA to feed-forward layers\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">lora_dropout</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,  # Dropout rate for LoRA parameters\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "# Apply LoRA to the model\n",
       "model = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Torchtune.apply_lora</span><span style=\"font-weight: bold\">(</span>model, config<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "# Freeze the base model parameters\n",
       "for param in <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model.base_model.parameters</span><span style=\"font-weight: bold\">()</span>:\n",
       "    param.requires_grad = <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "\n",
       "# Ensure only LoRA parameters are trainable\n",
       "for name, param in <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model.named_parameters</span><span style=\"font-weight: bold\">()</span>:\n",
       "    if <span style=\"color: #008000; text-decoration-color: #008000\">'lora'</span> in <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">name.lower</span><span style=\"font-weight: bold\">()</span>:\n",
       "        param.requires_grad = <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "    else:\n",
       "        param.requires_grad = <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "\n",
       "# Initialize the trainer with appropriate settings\n",
       "trainer = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Torchtune.create_trainer</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #800080; text-decoration-color: #800080\">model</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">args</span>=<span style=\"color: #800080; text-decoration-color: #800080\">training_args</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">train_dataset</span>=<span style=\"color: #800080; text-decoration-color: #800080\">train_dataset</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_dataset</span>=<span style=\"color: #800080; text-decoration-color: #800080\">eval_dataset</span>,\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "**Explanation**:\n",
       "\n",
       "- **Loading the Model**: The base model is loaded using Hugging Face's `AutoModelForMaskedLM`.\n",
       "\n",
       "- **Applying LoRA**: The `LoraConfig` is set up with parameters like rank, attention, and feed-forward settings. \n",
       "Torchtune's `apply_lora` method modifies the model to include LoRA adapters.\n",
       "\n",
       "- **Freezing Parameters**: The base model's parameters are frozen to prevent updates. Only LoRA parameters are left\n",
       "trainable by setting their `requires_grad` to `<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>`.\n",
       "\n",
       "- **Training Configuration**: The trainer is configured to handle the training process, focusing only on LoRA \n",
       "parameters.\n",
       "\n",
       "This approach ensures efficient fine-tuning by leveraging LoRA's parameter efficiency, making the process faster \n",
       "and less resource-intensive.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mAgent Answer:\u001b[0m Okay, so I'm trying to figure out how to ensure that only the LoRA parameters are trainable when I'm \n",
       "fine-tuning a model using the Torchtune project. I remember that LoRA stands for Low-Rank Adaptation, and it's a \n",
       "method used to efficiently fine-tune large language models by modifying only a small number of parameters. This is \n",
       "supposed to make the fine-tuning process faster and less resource-intensive compared to fine-tuning the entire \n",
       "model.\n",
       "\n",
       "First, I think I need to understand how LoRA works in the context of Torchtune. From what I recall, LoRA involves \n",
       "adding two small matrices to the attention layers of a transformer model. These matrices are typically low-rank, \n",
       "meaning they have a much smaller dimension than the original layers, which reduces the number of parameters that \n",
       "need to be updated during fine-tuning.\n",
       "\n",
       "So, when using Torchtune, I believe there's a specific way to apply LoRA. Maybe it's through a decorator or a \n",
       "configuration setting. I think the Hugging Face library, which Torchtune is built on, has a LoRA approach, and \n",
       "perhaps Torchtune provides a similar or integrated method.\n",
       "\n",
       "I remember seeing something about using the `LoraConfig` class in Hugging Face's `transformers` library. Maybe \n",
       "Torchtune has a similar configuration option. I should check if Torchtune has a `LoraConfig` or something \n",
       "equivalent. If it does, I can set that up to enable LoRA.\n",
       "\n",
       "Once LoRA is enabled, I need to make sure that only the LoRA parameters are trainable. That means the rest of the \n",
       "model's parameters should be frozen or not updated during training. I think in PyTorch, you can set `requires_grad`\n",
       "to \u001b[3;91mFalse\u001b[0m for the parameters you don't want to update. But with LoRA, perhaps Torchtune handles this automatically, \n",
       "so I just need to ensure that the LoRA parameters are the only ones with `requires_grad` set to \u001b[3;92mTrue\u001b[0m.\n",
       "\n",
       "I also recall that when using LoRA, you typically use an adapter or a similar mechanism to apply the low-rank \n",
       "transformations. In Torchtune, maybe this is done through a specific trainer or a callback. I should look into the \n",
       "Torchtune documentation to see if there's a recommended way to apply LoRA and control which parameters are \n",
       "trainable.\n",
       "\n",
       "Another thought: when initializing the model with LoRA, the base model's parameters are usually frozen. So, during \n",
       "the fine-tuning process, only the LoRA parameters are updated. I think this is done by setting the base model to \n",
       "evaluation mode or freezing its parameters. Maybe in Torchtune, this is handled by the `pl.Trainer` configuration \n",
       "or through some specific flags.\n",
       "\n",
       "I should also consider the training loop. In PyTorch Lightning, which Torchtune uses, the training loop \n",
       "automatically updates the parameters based on the optimizer. So, if I set up the model correctly with only LoRA \n",
       "parameters trainable, the optimizer should only update those.\n",
       "\n",
       "Wait, maybe I can use the `freeze` method provided by Torchtune or Hugging Face. For example, after applying LoRA, \n",
       "I can freeze the base model and unfreeze the LoRA layers. That way, during training, only the LoRA parameters are \n",
       "updated.\n",
       "\n",
       "I'm a bit confused about how exactly to apply LoRA in Torchtune. Let me think step by step. First, I load the base \n",
       "model, then apply the LoRA adapter to it. Then, I need to make sure that the base model's parameters are not \n",
       "trainable. So, I can loop through the model's named parameters and set `requires_grad` to \u001b[3;91mFalse\u001b[0m for all except the \n",
       "LoRA ones.\n",
       "\n",
       "Alternatively, maybe Torchtune provides a utility function to apply LoRA and freeze the base model automatically. I\n",
       "should check the Torchtune documentation or examples to see if there's a standard way to do this.\n",
       "\n",
       "Another point: when using LoRA, the model's forward pass is modified to include the LoRA transformations. So, I \n",
       "need to make sure that during the forward pass, the LoRA layers are active and their parameters are being used. I \n",
       "think this is handled by the LoRA implementation, but I should confirm.\n",
       "\n",
       "I also wonder about the optimizer and learning rate. Since only a small portion of parameters are being updated, \n",
       "maybe I need to adjust the learning rate for those parameters differently. But I think the optimizer in PyTorch \n",
       "Lightning handles this as long as the parameters are marked as trainable.\n",
       "\n",
       "In summary, the steps I think I need to take are:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Load the base model.\n",
       "\u001b[1;36m2\u001b[0m. Apply LoRA to the model, which adds the LoRA parameters.\n",
       "\u001b[1;36m3\u001b[0m. Freeze the base model's parameters so they aren't updated during training.\n",
       "\u001b[1;36m4\u001b[0m. Ensure that only the LoRA parameters are marked as trainable.\n",
       "\u001b[1;36m5\u001b[0m. Use the appropriate trainer configuration in Torchtune to handle the training loop.\n",
       "\n",
       "I should look up specific code examples or documentation from Torchtune that demonstrates applying LoRA and \n",
       "fine-tuning only those parameters. Maybe there's a specific decorator or function call that handles this setup for \n",
       "me.\n",
       "\n",
       "Wait, I think in Hugging Face's `transformers` library, they have a `set_frozen` method or something similar to \n",
       "freeze parts of the model. Maybe Torchtune has a similar utility. Alternatively, I can manually loop through the \n",
       "parameters and set `requires_grad` as needed.\n",
       "\n",
       "I'm also considering whether using the `\u001b[1;35mtorch.no_grad\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` context might help, but I think that's more for inference \n",
       "rather than training. So, probably not the right approach here.\n",
       "\n",
       "Another idea: when using LoRA, the adapter layers are typically added to the attention layers. So, in the model, \n",
       "these adapter layers are the only ones that should be trainable. Therefore, I can iterate over the model's modules \n",
       "and set `requires_grad` to \u001b[3;91mFalse\u001b[0m for all modules except the adapter ones.\n",
       "\n",
       "But how do I identify the adapter modules? Maybe they have a specific name or are part of a certain class. I should\n",
       "check the structure of the model after applying LoRA to see how the adapter layers are named or structured.\n",
       "\n",
       "Alternatively, perhaps Torchtune provides a way to automatically handle this, like a flag to enable LoRA and only \n",
       "train those parameters. That would be the most straightforward approach.\n",
       "\n",
       "I think I need to look into the Torchtune documentation or examples to see if there's a built-in method for \n",
       "applying LoRA and controlling trainability. If not, I can fall back to manually setting the parameters' \n",
       "`requires_grad` attribute.\n",
       "\n",
       "In any case, the key points are:\n",
       "\n",
       "- Apply LoRA to the model to add the small, trainable parameters.\n",
       "- Freeze the base model's parameters to prevent them from being updated.\n",
       "- Ensure that only the LoRA parameters are marked as trainable.\n",
       "\n",
       "I should also test this setup by checking the number of trainable parameters before and after training to confirm \n",
       "that only the LoRA parameters are being updated. That way, I can verify that my approach is correct.\n",
       "\n",
       "Another consideration: when using LoRA, sometimes the original attention weights are kept but scaled by the LoRA \n",
       "parameters. So, it's important that the base model's parameters remain unchanged during training, which is why \n",
       "freezing them is necessary.\n",
       "\n",
       "I think I've got a rough idea of the steps. Now, I should try to implement this in code, using Torchtune's APIs, \n",
       "and see if it works as expected. If I run into issues, I can refer back to the documentation or seek help from \n",
       "community resources.\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mthink\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "To ensure only LoRA parameters are trainable when fine-tuning using Torchtune, follow these steps:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Load the Base Model**: Start by loading the pre-trained model you wish to fine-tune.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Apply LoRA**: Use Torchtune's or Hugging Face's `LoraConfig` to apply LoRA to the model. This adds LoRA \n",
       "adapter layers to the attention modules.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Freeze the Base Model**: After applying LoRA, freeze the base model's parameters to prevent them from being \n",
       "updated during training. This can be done by setting `requires_grad` to `\u001b[3;91mFalse\u001b[0m` for all parameters except the LoRA \n",
       "ones.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Configure the Trainer**: Use Torchtune's trainer with appropriate settings to handle the training loop, \n",
       "ensuring only LoRA parameters are updated.\n",
       "\n",
       "Here's a code example demonstrating these steps:\n",
       "\n",
       "```python\n",
       "from torchtune import Torchtune\n",
       "from transformers import LoraConfig, AutoModelForMaskedLM\n",
       "\n",
       "# Load the base model\n",
       "model = \u001b[1;35mAutoModelForMaskedLM.from_pretrained\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'bert-base-uncased'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Apply LoRA configuration\n",
       "config = \u001b[1;35mLoraConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mr\u001b[0m=\u001b[1;36m8\u001b[0m,  # Rank of the LoRA adaptation\n",
       "    \u001b[33mlora_attention\u001b[0m=\u001b[3;91mFalse\u001b[0m,  # Apply LoRA to attention layers\n",
       "    \u001b[33mlora_feedforward\u001b[0m=\u001b[3;92mTrue\u001b[0m,  # Apply LoRA to feed-forward layers\n",
       "    \u001b[33mlora_dropout\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m,  # Dropout rate for LoRA parameters\n",
       "\u001b[1m)\u001b[0m\n",
       "\n",
       "# Apply LoRA to the model\n",
       "model = \u001b[1;35mTorchtune.apply_lora\u001b[0m\u001b[1m(\u001b[0mmodel, config\u001b[1m)\u001b[0m\n",
       "\n",
       "# Freeze the base model parameters\n",
       "for param in \u001b[1;35mmodel.base_model.parameters\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\n",
       "    param.requires_grad = \u001b[3;91mFalse\u001b[0m\n",
       "\n",
       "# Ensure only LoRA parameters are trainable\n",
       "for name, param in \u001b[1;35mmodel.named_parameters\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\n",
       "    if \u001b[32m'lora'\u001b[0m in \u001b[1;35mname.lower\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\n",
       "        param.requires_grad = \u001b[3;92mTrue\u001b[0m\n",
       "    else:\n",
       "        param.requires_grad = \u001b[3;91mFalse\u001b[0m\n",
       "\n",
       "# Initialize the trainer with appropriate settings\n",
       "trainer = \u001b[1;35mTorchtune.create_trainer\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[35mmodel\u001b[0m,\n",
       "    \u001b[33margs\u001b[0m=\u001b[35mtraining_args\u001b[0m,\n",
       "    \u001b[33mtrain_dataset\u001b[0m=\u001b[35mtrain_dataset\u001b[0m,\n",
       "    \u001b[33meval_dataset\u001b[0m=\u001b[35meval_dataset\u001b[0m,\n",
       "\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "**Explanation**:\n",
       "\n",
       "- **Loading the Model**: The base model is loaded using Hugging Face's `AutoModelForMaskedLM`.\n",
       "\n",
       "- **Applying LoRA**: The `LoraConfig` is set up with parameters like rank, attention, and feed-forward settings. \n",
       "Torchtune's `apply_lora` method modifies the model to include LoRA adapters.\n",
       "\n",
       "- **Freezing Parameters**: The base model's parameters are frozen to prevent updates. Only LoRA parameters are left\n",
       "trainable by setting their `requires_grad` to `\u001b[3;92mTrue\u001b[0m`.\n",
       "\n",
       "- **Training Configuration**: The trainer is configured to handle the training process, focusing only on LoRA \n",
       "parameters.\n",
       "\n",
       "This approach ensures efficient fine-tuning by leveraging LoRA's parameter efficiency, making the process faster \n",
       "and less resource-intensive.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_agent = Agent(client,\n",
    "                     model=MODEL_ID, \n",
    "                     instructions=\"You are a helpful assistant that can answer questions about the Torchtune project.\")\n",
    "for example in examples:\n",
    "    simple_session_id = simple_agent.create_session(session_name=f\"simple_session_{uuid.uuid4()}\")\n",
    "    response = simple_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"input_query\"]\n",
    "            }\n",
    "        ],\n",
    "        session_id=simple_session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    rich.print(f\"[bold cyan]Question:[/bold cyan] {example['input_query']}\")\n",
    "    rich.print(f\"[bold yellow]Agent Answer:[/bold yellow] {response.output_message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Evaluate Agent Responses\n",
    "Let's gather up the agent's logs and evaluate the agent's performance. We can see that our agent's response is quite bad and off from the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'detail': 'Invalid value: Pass OpenAI API Key in the header X-LlamaStack-Provider-Data as { \"openai_api_key\": <your api key>}'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m         eval_rows\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: examples[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_query\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: examples[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: turn\u001b[38;5;241m.\u001b[39moutput_message\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m      9\u001b[0m         })\n\u001b[1;32m     11\u001b[0m scoring_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbraintrust::factuality\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 14\u001b[0m scoring_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m pprint(scoring_response)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/llama_stack_client/resources/scoring.py:76\u001b[0m, in \u001b[0;36mScoringResource.score\u001b[0;34m(self, input_rows, scoring_functions, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscore\u001b[39m(\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScoringScoreResponse:\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    Score a list of rows.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/scoring/score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_rows\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscoring_functions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscoring_score_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScoringScoreParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mScoringScoreResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/llama_stack_client/_base_client.py:1222\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1210\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1218\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1219\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1220\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1221\u001b[0m     )\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/llama_stack_client/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1028\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1030\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1031\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'detail': 'Invalid value: Pass OpenAI API Key in the header X-LlamaStack-Provider-Data as { \"openai_api_key\": <your api key>}'}"
     ]
    }
   ],
   "source": [
    "eval_rows = []\n",
    "for i, session_id in enumerate(simple_agent.sessions):\n",
    "    session_response = client.agents.session.retrieve(agent_id=simple_agent.agent_id, session_id=session_id)\n",
    "    for turn in session_response.turns:\n",
    "        eval_rows.append({\n",
    "            \"input_query\": examples[i][\"input_query\"],\n",
    "            \"expected_answer\": examples[i][\"expected_answer\"],\n",
    "            \"generated_answer\": turn.output_message.content,\n",
    "        })\n",
    "\n",
    "scoring_params = {\n",
    "    \"braintrust::factuality\": None,\n",
    "}\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=eval_rows,\n",
    "    scoring_functions=scoring_params,\n",
    ")\n",
    "pprint(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Agent\n",
    "\n",
    "Now, let's see how we can improve the agent's performance by adding a search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = Agent(client, \n",
    "                     model=MODEL_ID,\n",
    "                     instructions=\"You are a helpful assistant that can answer questions about the Torchtune project. You should always use the search tool to answer questions.\",\n",
    "                     tools=[\"builtin::websearch\"])\n",
    "for example in examples:\n",
    "    search_session_id = search_agent.create_session(session_name=f\"search_session_{uuid.uuid4()}\")\n",
    "    response = search_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"input_query\"]\n",
    "            }\n",
    "        ],\n",
    "        session_id=search_session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    rich.print(f\"[bold cyan]Question:[/bold cyan] {example['input_query']}\")\n",
    "    rich.print(f\"[bold yellow]Agent Answer:[/bold yellow] {response.output_message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Evaluate Agent Responses\n",
    "\n",
    "We can see that with a search tool, the agent's performance is much better, and have less hallucinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rows = []\n",
    "for i, session_id in enumerate(search_agent.sessions):\n",
    "    session_response = client.agents.session.retrieve(agent_id=search_agent.agent_id, session_id=session_id)\n",
    "    for turn in session_response.turns:\n",
    "        eval_rows.append({\n",
    "            \"input_query\": examples[i][\"input_query\"],\n",
    "            \"expected_answer\": examples[i][\"expected_answer\"],\n",
    "            \"generated_answer\": turn.output_message.content,\n",
    "        })\n",
    "\n",
    "scoring_params = {\n",
    "    \"braintrust::factuality\": None,\n",
    "}\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=eval_rows,\n",
    "    scoring_functions=scoring_params,\n",
    ")\n",
    "pprint(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RAG Agent\n",
    "\n",
    "Now, let's see how we can improve the agent's performance by adding a RAG tool that explicitly retrieves information from Torchtune's documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types import Document\n",
    "urls = [\n",
    "    \"memory_optimizations.rst\",\n",
    "    \"chat.rst\",\n",
    "    \"llama3.rst\",\n",
    "    \"datasets.rst\",\n",
    "    \"qat_finetune.rst\",\n",
    "    \"lora_finetune.rst\",\n",
    "]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "\n",
    "vector_providers = [\n",
    "    provider for provider in client.providers.list() if provider.api == \"vector_io\"\n",
    "]\n",
    "selected_vector_provider = vector_providers[0]\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n",
    "\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=MODEL_ID,\n",
    "    instructions=\"You are a helpful assistant that can answer questions about the Torchtune project. You should always use the RAG tool to answer questions.\",\n",
    "    tools=[{\n",
    "        \"name\": \"builtin::rag\",\n",
    "        \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
    "    }],\n",
    ")\n",
    "\n",
    "for example in examples:\n",
    "    rag_session_id = rag_agent.create_session(session_name=f\"rag_session_{uuid.uuid4()}\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"input_query\"]\n",
    "            }\n",
    "        ],\n",
    "        session_id=rag_session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    rich.print(f\"[bold cyan]Question:[/bold cyan] {example['input_query']}\")\n",
    "    rich.print(f\"[bold yellow]Agent Answer:[/bold yellow] {response.output_message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rows = []\n",
    "for i, session_id in enumerate(rag_agent.sessions):\n",
    "    session_response = client.agents.session.retrieve(agent_id=rag_agent.agent_id, session_id=session_id)\n",
    "    for turn in session_response.turns:\n",
    "        eval_rows.append({\n",
    "            \"input_query\": examples[i][\"input_query\"],\n",
    "            \"expected_answer\": examples[i][\"expected_answer\"],\n",
    "            \"generated_answer\": turn.output_message.content,\n",
    "        })\n",
    "\n",
    "scoring_params = {\n",
    "    \"braintrust::factuality\": None,\n",
    "}\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=eval_rows,\n",
    "    scoring_functions=scoring_params,\n",
    ")\n",
    "pprint(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep dive into RAG Tool Performance\n",
    "- Now, let's take a closer look at how the RAG tool is doing, specifically on the second example where the agent's answer is not correct on identifying what DoRA stands for. \n",
    "- Notice that the issue lies with the retrieval step, where the retrieved document is not relevant to the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_response = client.agents.session.retrieve(agent_id=rag_agent.agent_id, session_id=rag_agent.sessions[1])\n",
    "pprint(session_response.turns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Improved RAG with Long Context\n",
    "\n",
    "- Instead of performing reteival tool, we send documents as attachments to the agent and let it use the entire document context. \n",
    "- Note how that the model is able to understand the entire context from documentation and answers the question with better factuality with improved retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"memory_optimizations.rst\",\n",
    "    \"chat.rst\",\n",
    "    \"llama3.rst\",\n",
    "    \"datasets.rst\",\n",
    "    \"qat_finetune.rst\",\n",
    "    \"lora_finetune.rst\",\n",
    "]\n",
    "\n",
    "attachments = [\n",
    "    {\n",
    "        \"content\": {\n",
    "            \"uri\": f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        },\n",
    "        \"mime_type\": \"text/plain\",\n",
    "    }\n",
    "\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "\n",
    "rag_attachment_agent = Agent(\n",
    "    client,\n",
    "    model=MODEL_ID,\n",
    "    instructions=\"You are a helpful assistant that can answer questions about the Torchtune project. Use context from attached documentation for Torchtune to answer questions.\",\n",
    ")\n",
    "\n",
    "for example in examples:\n",
    "    session_id = rag_attachment_agent.create_session(session_name=f\"rag_attachment_session_{uuid.uuid4()}\")\n",
    "    response = rag_attachment_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"input_query\"]\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        documents=attachments,\n",
    "        stream=False\n",
    "    )\n",
    "    rich.print(f\"[bold cyan]Question:[/bold cyan] {example['input_query']}\")\n",
    "    rich.print(f\"[bold yellow]Agent Answer:[/bold yellow] {response.output_message.content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rows = []\n",
    "for i, session_id in enumerate(rag_attachment_agent.sessions):\n",
    "    session_response = client.agents.session.retrieve(agent_id=rag_attachment_agent.agent_id, session_id=session_id)\n",
    "    for turn in session_response.turns:\n",
    "        eval_rows.append({\n",
    "            \"input_query\": examples[i][\"input_query\"],\n",
    "            \"expected_answer\": examples[i][\"expected_answer\"],\n",
    "            \"generated_answer\": turn.output_message.content,\n",
    "        })\n",
    "\n",
    "scoring_params = {\n",
    "    \"braintrust::factuality\": None,\n",
    "}\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=eval_rows,\n",
    "    scoring_functions=scoring_params,\n",
    ")\n",
    "pprint(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
