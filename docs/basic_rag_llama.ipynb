{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b708f4-4fe3-4cc8-9b0d-623100e83317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-stack-client==0.2.1 faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa6179b-bf45-4f33-b9b2-9d4c23886257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types.shared_params.document import Document as RAGDocument\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger as AgentEventLogger\n",
    "import os\n",
    "\n",
    "# Initialize the client\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15262b6a-897d-43e6-8e9c-11c782f359d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_id = \"my_documents\"\n",
    "\n",
    "response = client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=\"faiss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3396a877-2a29-427f-b516-12ab851537ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"memory_optimizations.rst\", \"chat.rst\", \"llama3.rst\"]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb00e33-7e00-40ed-8b68-e9ad606f2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=\"Qwen/Qwen3-14B\",\n",
    "    # Define instructions for the agent (system prompt)\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    enable_session_persistence=False,\n",
    "    # Define tools available to the agent\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\n",
    "                \"vector_db_ids\": [vector_db_id],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7453dd3-9b65-49d2-b2fd-f272da82a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = rag_agent.create_session(\"test-session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22724b75-0eda-457d-bca9-a684732ae8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"How to optimize memory usage in torchtune? use the knowledge_search tool to get information.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7774fa-971a-4ea4-a504-0243b9bd37a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User> How to optimize memory usage in torchtune? use the knowledge_search tool to get information.\n",
      "\u001b[33minference> \u001b[0m\u001b[33m<think>\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mOkay\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m is\u001b[0m\u001b[33m asking\u001b[0m\u001b[33m how\u001b[0m\u001b[33m to\u001b[0m\u001b[33m optimize\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m usage\u001b[0m\u001b[33m in\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m.\u001b[0m\u001b[33m I\u001b[0m\u001b[33m need\u001b[0m\u001b[33m to\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m knowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m tool\u001b[0m\u001b[33m to\u001b[0m\u001b[33m find\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Let\u001b[0m\u001b[33m me\u001b[0m\u001b[33m start\u001b[0m\u001b[33m by\u001b[0m\u001b[33m understanding\u001b[0m\u001b[33m what\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m is\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Tor\u001b[0m\u001b[33mcht\u001b[0m\u001b[33mune\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m library\u001b[0m\u001b[33m for\u001b[0m\u001b[33m training\u001b[0m\u001b[33m and\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33muning\u001b[0m\u001b[33m large\u001b[0m\u001b[33m-scale\u001b[0m\u001b[33m models\u001b[0m\u001b[33m,\u001b[0m\u001b[33m probably\u001b[0m\u001b[33m related\u001b[0m\u001b[33m to\u001b[0m\u001b[33m Py\u001b[0m\u001b[33mT\u001b[0m\u001b[33morch\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Memory\u001b[0m\u001b[33m optimization\u001b[0m\u001b[33m in\u001b[0m\u001b[33m such\u001b[0m\u001b[33m contexts\u001b[0m\u001b[33m usually\u001b[0m\u001b[33m involves\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m like\u001b[0m\u001b[33m gradient\u001b[0m\u001b[33m checkpoint\u001b[0m\u001b[33ming\u001b[0m\u001b[33m,\u001b[0m\u001b[33m mixed\u001b[0m\u001b[33m precision\u001b[0m\u001b[33m training\u001b[0m\u001b[33m,\u001b[0m\u001b[33m or\u001b[0m\u001b[33m optimizing\u001b[0m\u001b[33m data\u001b[0m\u001b[33m loading\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m user\u001b[0m\u001b[33m wants\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m or\u001b[0m\u001b[33m best\u001b[0m\u001b[33m practices\u001b[0m\u001b[33m for\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Since\u001b[0m\u001b[33m I\u001b[0m\u001b[33m don\u001b[0m\u001b[33m't\u001b[0m\u001b[33m have\u001b[0m\u001b[33m prior\u001b[0m\u001b[33m knowledge\u001b[0m\u001b[33m on\u001b[0m\u001b[33m this\u001b[0m\u001b[33m,\u001b[0m\u001b[33m I\u001b[0m\u001b[33m should\u001b[0m\u001b[33m use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m knowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m function\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m query\u001b[0m\u001b[33m parameter\u001b[0m\u001b[33m should\u001b[0m\u001b[33m be\u001b[0m\u001b[33m a\u001b[0m\u001b[33m natural\u001b[0m\u001b[33m language\u001b[0m\u001b[33m question\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Let\u001b[0m\u001b[33m me\u001b[0m\u001b[33m phrase\u001b[0m\u001b[33m it\u001b[0m\u001b[33m as\u001b[0m\u001b[33m \"\u001b[0m\u001b[33mHow\u001b[0m\u001b[33m to\u001b[0m\u001b[33m optimize\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m usage\u001b[0m\u001b[33m in\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m?\"\u001b[0m\u001b[33m to\u001b[0m\u001b[33m get\u001b[0m\u001b[33m the\u001b[0m\u001b[33m most\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m results\u001b[0m\u001b[33m.\u001b[0m\u001b[33m I\u001b[0m\u001b[33m'll\u001b[0m\u001b[33m make\u001b[0m\u001b[33m sure\u001b[0m\u001b[33m the\u001b[0m\u001b[33m function\u001b[0m\u001b[33m call\u001b[0m\u001b[33m is\u001b[0m\u001b[33m correctly\u001b[0m\u001b[33m formatted\u001b[0m\u001b[33m in\u001b[0m\u001b[33m JSON\u001b[0m\u001b[33m within\u001b[0m\u001b[33m the\u001b[0m\u001b[33m tool\u001b[0m\u001b[33m_call\u001b[0m\u001b[33m tags\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m</think>\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33m<tool_call>\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m{\"\u001b[0m\u001b[33mname\u001b[0m\u001b[33m\":\u001b[0m\u001b[33m \"\u001b[0m\u001b[33mknowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m\",\u001b[0m\u001b[33m \"\u001b[0m\u001b[33marguments\u001b[0m\u001b[33m\":\u001b[0m\u001b[33m {\"\u001b[0m\u001b[33mquery\u001b[0m\u001b[33m\":\u001b[0m\u001b[33m \"\u001b[0m\u001b[33mHow\u001b[0m\u001b[33m to\u001b[0m\u001b[33m optimize\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m usage\u001b[0m\u001b[33m in\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m?\"\u001b[0m\u001b[33m}}\n",
      "\u001b[0m\u001b[33m</tool_call>\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "from llama_stack_client.lib.agents.event_logger import EventLogger as AgentEventLogger\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(f\"User> {prompt}\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
